{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import shuffle\n",
    "import copy\n",
    "#from statistics import mode\n",
    "from scipy.stats import mode\n",
    "from sklearn.model_selection import KFold #To shuffle our data into K-folds\n",
    "from sklearn.metrics import confusion_matrix #To calculate confusion maxtrix\n",
    "import math\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load our data\n",
    "def loadData(name):\n",
    "    data = []\n",
    "    with open(name) as openfile:\n",
    "        readfile = csv.reader(openfile, delimiter=',')\n",
    "        for line in readfile:\n",
    "            if len(line) > 1:\n",
    "                data.append(line)\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class classificationTree():\n",
    "    def __init__(self, n_min=5): #Create the tree with leaf size of n_min % of total data.\n",
    "        self.n_min = n_min\n",
    "        self.root = {}\n",
    "\n",
    "    #Splits into two groups for discreet data.\n",
    "    def __splitGroupDiscreet(self, data, axis, name):\n",
    "        #Place data into respective groups based on name.\n",
    "        group1 = []\n",
    "        group2 = []\n",
    "        for entry in data:\n",
    "            if entry[axis] == name:\n",
    "                group1.append(entry)\n",
    "            else:\n",
    "                group2.append(entry)\n",
    "        return(group1, group2)\n",
    "    \n",
    "    #Information gain of splitting on data[axis] = name\n",
    "    def __infoGainDiscreet(self, data, axis, name):\n",
    "        group1, group2 = self.__splitGroupDiscreet(data, axis, name)\n",
    "        group1_entropy = self.__entropy(group1) #Calculate the entropies\n",
    "        group2_entropy = self.__entropy(group2) #Calculate the entropies\n",
    "        resultEntropy = (len(group1)/len(data))*group1_entropy + (len(group2)/len(data))*group2_entropy #Ave entropy\n",
    "        return(self.__entropy(data) - resultEntropy)\n",
    "    \n",
    "    #Splits into two groups for continuous data.\n",
    "    def __splitGroupContinuous(self, data, axis, min_val):\n",
    "        group1 = []\n",
    "        group2 = []\n",
    "        for entry in data:\n",
    "            if len(min_val) > 0 and float(entry[axis]) > float(min_val):\n",
    "                group1.append(entry)\n",
    "            else:\n",
    "                group2.append(entry)\n",
    "                \n",
    "        return(group1, group2)\n",
    "    \n",
    "    #Information gain of splitting on value > min_val\n",
    "    def __infoGainContinuous(self, data, axis, min_val):\n",
    "        group1, group2 = self.__splitGroupContinuous(data, axis, str(min_val))\n",
    "        group1_entropy = self.__entropy(group1) #Calculate the entropies\n",
    "        group2_entropy = self.__entropy(group2) #Calculate the entropies\n",
    "        resultEntropy = (len(group1)/len(data))*group1_entropy + (len(group2)/len(data))*group2_entropy #Ave entropy\n",
    "        return(self.__entropy(data) - resultEntropy)\n",
    "    \n",
    "    #Entropy of a group with respect to our targets (self.labels)\n",
    "    def __entropy(self, group):\n",
    "        totalcount = len(group) #Total number of inputs\n",
    "        entropy = 0 #Total entropy.\n",
    "        true = 0\n",
    "\n",
    "        #Calculated the weighted entropy of each label.   \n",
    "        for label in self.labels:\n",
    "            true = [x for x in group if label in x[-1]]\n",
    "            if len(true) != 0: #Log of 0.0 is undefined.\n",
    "                entropy -= (len(true)/totalcount) * math.log((len(true)/totalcount), 2)\n",
    "        return(entropy)\n",
    "        \n",
    "    \n",
    "    #Finds the feature to split with the highest information gain.\n",
    "    #For discreet data, we use the median as the splitting category.\n",
    "    #For continuous data, we use the average\n",
    "    #Returns the column(feature), value\n",
    "    def __find_best_feature(self, data):\n",
    "        highest_info_gain = 0.0 #This tracks the highest we found\n",
    "        highest_column = 0 #This tracks the feature we should split by.\n",
    "        splitby = \"\" #This tracks the name (discreet) or min_split (continuous)\n",
    "        \n",
    "        for i in range(len(data[0])-1): #For each feature (column) minus label.\n",
    "            #Determine the type of feature.\n",
    "            featuretype = \"\"\n",
    "            if 'int' in str(type(data[0][i])) or 'float' in str(type(data[0][i])):\n",
    "                featuretype = \"continuous\"\n",
    "            elif 'str' in str(type(data[0][i])):\n",
    "                featuretype = \"discreet\"\n",
    "            \n",
    "            #If our feature is discreet, we check info gain on each name(type)\n",
    "            if featuretype == \"discreet\":\n",
    "                #Get only unique categories.\n",
    "                possibility = set()\n",
    "                for j in range(len(data)):\n",
    "                    possibility.add(data[j][i])\n",
    "                #Now iterate over unique names\n",
    "                for name in possibility:\n",
    "                    info_gain = self.__infoGainDiscreet(data, i, name)\n",
    "                    if(info_gain > highest_info_gain): #If we exceeded existin info_gain\n",
    "                        highest_info_gain = info_gain #Update highest values.\n",
    "                        highest_column = i\n",
    "                        splitby = name\n",
    "\n",
    "            #If the feature is continuous:\n",
    "            #\"For determining the optimal threshold for splitting you will need to search over\n",
    "            #all possible thresholds for a given feature\". This is slow. Esp for tasks \n",
    "            #such as spambase with 4k+ values over 20+ features\n",
    "            #We will use the mean value instead rather than search over the range.\n",
    "            elif featuretype == \"continuous\":\n",
    "                mean = 0\n",
    "                for j in range(len(data)):\n",
    "                    mean += data[j][i]\n",
    "                mean = (mean/len(data))\n",
    "                info_gain = self.__infoGainContinuous(data, i, mean)\n",
    "                if(info_gain > highest_info_gain): #If we exceeded existin info_gain\n",
    "                    highest_info_gain = info_gain #Update highest values.\n",
    "                    highest_column = i\n",
    "                    splitby = str(mean)\n",
    "\n",
    "        return(highest_column, splitby, featuretype)\n",
    "        \n",
    "    \n",
    "    #Given a node, find it's correct splits, and assign values.\n",
    "    def __splitNode(self, node):\n",
    "        \n",
    "        #Find the optimal splitting strategy.\n",
    "        column, splitby, featuretype = self.__find_best_feature(node[\"data\"])\n",
    "        node[\"column\"] = column\n",
    "        node[\"splitby\"] = splitby\n",
    "        \n",
    "        #Obtain the splits into group1 and group2\n",
    "        group1 = None\n",
    "        group2 = None\n",
    "        if featuretype == \"discreet\":\n",
    "            node[\"featuretype\"] = \"discreet\"\n",
    "            group1, group2 = self.__splitGroupDiscreet(node[\"data\"], column, splitby)\n",
    "        else:\n",
    "            node[\"featuretype\"] = \"continuous\"\n",
    "            group1, group2 = self.__splitGroupContinuous(node[\"data\"], column, splitby)\n",
    "        \n",
    "        #If one group has size <= self.n_min, we stop.\n",
    "        #Also stop if all features have the same value.\n",
    "        #Store the \"mode\" feature in the yes/no.\n",
    "        if len(group1) == 0:\n",
    "            node[\"yes\"] = None\n",
    "        elif len(group1) <= self.n_min: #If we have fewer than our minimum\n",
    "            node[\"yes\"] = mode([x[-1] for x in group1])[0][0] #The mode feature is our \"yes value\"\n",
    "        elif len(set([x[-1] for x in group1])) == 1: #If the number of unique features is only 1\n",
    "            node[\"yes\"] = mode([x[-1] for x in group1])[0][0]\n",
    "        else: #If we still need to continue, then we recurse.\n",
    "            node[\"yes\"] = {\"data\":group1} \n",
    "            self.__splitNode(node[\"yes\"]) #Recurse on this side.\n",
    "\n",
    "        if len(group2) == 0:\n",
    "            node[\"no\"] = None\n",
    "        elif len(group2) <= self.n_min:\n",
    "            node[\"no\"] = mode([x[-1] for x in group2])[0][0]\n",
    "        elif len(set([x[-1] for x in group2])) == 1:\n",
    "            node[\"no\"] = mode([x[-1] for x in group2])[0][0]\n",
    "        else:\n",
    "            node[\"no\"] = {\"data\":group2}\n",
    "            self.__splitNode(node[\"no\"]) #Recurse on this side.\n",
    "        return\n",
    "\n",
    "        \n",
    "    #Fit the data into our tree.\n",
    "    #Continuous data should be numerical. Categorical should be string.\n",
    "    def fit(self, data, label):\n",
    "        data = copy.deepcopy(data)\n",
    "        label = copy.deepcopy(label)\n",
    "        #Attach the labels to the end of our data.\n",
    "        for i in range(len(data)):\n",
    "            data[i].append(label[i])\n",
    "        self.labels = list(set(label))   #These are the unique labels.\n",
    "        self.size = len(data) #size of our dataset\n",
    "        self.n_min = (self.n_min * self.size)/100 #Minimum leaf size.\n",
    "        \n",
    "        #Use ID3 algorithm. \n",
    "        #1. Create a root node.\n",
    "        self.root = {\"data\":data}\n",
    "        #Recursively split this node by finding it's left and right.\n",
    "        self.__splitNode(self.root)\n",
    "        \n",
    "    #Recursively querys the node until we get our answer.\n",
    "    def __checkValue(self, node, query):\n",
    "        answer = \"\"\n",
    "        if(node[\"featuretype\"] == \"continuous\"): #Get the featureType.\n",
    "            if(query[node[\"column\"]] > float(node[\"splitby\"])): #Do the corresponding test.\n",
    "                #Check if this node is the final.\n",
    "                if \"dict\" in str(type(node[\"yes\"])): \n",
    "                    answer = self.__checkValue(node[\"yes\"], query)\n",
    "                else:\n",
    "                    answer = node[\"yes\"]\n",
    "            else:\n",
    "                #Check if this node is the final.\n",
    "                if \"dict\" in str(type(node[\"no\"])): \n",
    "                    answer = self.__checkValue(node[\"no\"], query)\n",
    "                else:\n",
    "                    answer = node[\"no\"]\n",
    "        elif(node[\"featuretype\"] == \"discreet\"):\n",
    "            if(query[node[\"column\"]] == node[\"splitby\"]):\n",
    "                #Check if this node is the final.\n",
    "                if \"dict\" in str(type(node[\"yes\"])): \n",
    "                    answer = self.__checkValue(node[\"yes\"], query)\n",
    "                else:\n",
    "                    answer = node[\"yes\"]\n",
    "            else:\n",
    "                #Check if this node is the final.\n",
    "                if \"dict\" in str(type(node[\"no\"])): \n",
    "                    answer = self.__checkValue(node[\"no\"], query)\n",
    "                else:\n",
    "                    answer = node[\"no\"]\n",
    "        return(answer)\n",
    "\n",
    "    #The driver to call self.__checkValue with.\n",
    "    def predict(self, query):\n",
    "        return(self.__checkValue(self.root, query))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification of flowers based on features (iris.csv dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the iris dataset.\n",
    "data = loadData(\"iris.csv\")\n",
    "label = [x[-1] for x in data] #Take out the labels.\n",
    "data = [x[:-1] for x in data]\n",
    "#Set the correct data types.\n",
    "for i in range(len(data)):\n",
    "    data[i][0] = float(data[i][0])\n",
    "    data[i][1] = float(data[i][1])\n",
    "    data[i][2] = float(data[i][2])\n",
    "    data[i][3] = float(data[i][3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomato721/anaconda3/lib/python3.7/site-packages/scipy/stats/stats.py:245: RuntimeWarning: The input array could not be properly checked for nan values. nan values will be ignored.\n",
      "  \"values. nan values will be ignored.\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "#Create and fit our classification tree.\n",
    "tree = classificationTree(n_min = 5) #n_min = minimum percentage of data as leafnode size.\n",
    "tree.fit(data,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Iris-virginica'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We can test on one value.\n",
    "tree.predict([6.4, 2.8, 5.7, 2.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the accuracy over 10 folds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg accuracy over 10 folds for n_min 5 : 93.33333333333333\n",
      "Avg std5 : 7.888106377466154\n",
      "Avg accuracy over 10 folds for n_min 10 : 90.66666666666667\n",
      "Avg std10 : 8.537498983243799\n",
      "Avg accuracy over 10 folds for n_min 15 : 92.66666666666666\n",
      "Avg std15 : 7.571877794400365\n",
      "Avg accuracy over 10 folds for n_min 20 : 92.66666666666666\n",
      "Avg std20 : 7.571877794400365\n"
     ]
    }
   ],
   "source": [
    "n_splits=10 #10 folds\n",
    "\n",
    "n_min_tests = [5,10,15,20] #For these different n_min parameters\n",
    "for n_min in n_min_tests:\n",
    "    \n",
    "    kf = KFold(n_splits) #Use Kfolds to generate the test folds.\n",
    "    count = 0\n",
    "    accuracy = []\n",
    "    for train, test in kf.split(data):\n",
    "        #Get the training data ready for each fold\n",
    "        training_data = []\n",
    "        training_label = []\n",
    "        for i in train:\n",
    "            training_data.append(data[i])\n",
    "            training_label.append(label[i])\n",
    "        #Create our model for each fold, \n",
    "        kfoldmodel = classificationTree(n_min)\n",
    "        kfoldmodel.fit(training_data, training_label)\n",
    "\n",
    "        #Predict on the test labels and collect results.\n",
    "        test_label = []\n",
    "        test_label_predicted = []\n",
    "        for i in test:\n",
    "            test_label.append(label[i])\n",
    "            test_label_predicted.append(kfoldmodel.predict(data[i]))\n",
    "\n",
    "        #Calculate accuracy\n",
    "        total = len(test_label)\n",
    "        correct = 0\n",
    "        for i in range(total):\n",
    "            if(test_label[i] == test_label_predicted[i]):\n",
    "                correct += 1\n",
    "        count += 1\n",
    "        accuracy.append((correct*100)/total)\n",
    "    accuracy = np.array(accuracy)\n",
    "    \n",
    "    print(\"Avg accuracy over 10 folds for n_min \"+str(n_min)+\" :\", np.mean(accuracy))\n",
    "    print(\"Avg std\"+str(n_min)+\" :\", np.std(accuracy))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsuprisingly, the best performing model was the lowest n_min percentage of 5% (Since the tree can be more granular). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Based on the best n_min value (5), create a class confusion matrix using ten-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[46  0  4]\n",
      " [ 0 50  0]\n",
      " [ 1  6 43]]\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits)\n",
    "confusion_array = np.zeros((3,3), dtype=int) #initialize our confusion_array\n",
    "for train, test in kf.split(data):\n",
    "    #Get the training data ready for each fold\n",
    "    training_data = []\n",
    "    training_label = []\n",
    "    for i in train:\n",
    "        training_data.append(data[i])\n",
    "        training_label.append(label[i])\n",
    "    #Create our model for each fold, \n",
    "    kfoldmodel = classificationTree(n_min)\n",
    "    kfoldmodel.fit(training_data, training_label)\n",
    "\n",
    "    #Predict on the test labels and collect results.\n",
    "    test_label = []\n",
    "    test_label_predicted = []\n",
    "    for i in test:\n",
    "        test_label.append(label[i])\n",
    "        test_label_predicted.append(kfoldmodel.predict(data[i]))\n",
    "    #Sum over the confusion arrays since we use a test set segment out of total\n",
    "    confusion_array += confusion_matrix(test_label, test_label_predicted, labels=[\"Iris-virginica\", \"Iris-setosa\", \"Iris-versicolor\"])\n",
    "\n",
    "print(confusion_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confusion matrix can be understood as:\n",
    "\n",
    "for Iris-virginica we predicted 46 as Iris-virginica, 0 as Iris-setosa and 4 as Iris-versicolor.\n",
    "\n",
    "for Iris-setosa we predicted 0 as Iris-virginica, 50 as Iris-setosa and 0 as Iris-versicolor.\n",
    "\n",
    "for Iris-versicolor we predicted 1 as Iris-virginica, 6 as Iris-setosa and 43 as Iris-versicolor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification of spam (spambase.csv dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the spam dataset.\n",
    "data = loadData(\"spambase.csv\")\n",
    "shuffle(data) #Shuffle our data\n",
    "label = [x[-1] for x in data] #Take out the labels.\n",
    "data = [x[:-1] for x in data]\n",
    "\n",
    "\n",
    "#Set the correct data types.\n",
    "for i in range(len(data)):\n",
    "    for j in range(len(data[0])):\n",
    "        data[i][j] = float(data[i][j])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create and fit our classification tree.\n",
    "tree = classificationTree(n_min = 5) #n_min = minimum percentage of data as leafnode size.\n",
    "tree.fit(data,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.predict([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.86, 0.0, 0.0, 0.0, 0.0, 0.0, 3.73, 0.0, 1.86, 0.0, 0.93, 3.73, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.93, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.173, 0.0, 0.0, 0.0, 0.0, 1.9, 5.0, 38.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the accuracy over 10 folds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% Avg accuracy for n_min 5 : 89.89394510987458\n",
      "% Avg std of accuracy for n_min 5 : 1.2828025913491792\n",
      "% Avg accuracy for n_min 10 : 89.56771668395737\n",
      "% Avg std of accuracy for n_min 10 : 1.2198388074607098\n",
      "% Avg accuracy for n_min 15 : 85.95949259643496\n",
      "% Avg std of accuracy for n_min 15 : 1.3832206029310827\n",
      "% Avg accuracy for n_min 20 : 85.93775346600019\n",
      "% Avg std of accuracy for n_min 20 : 1.3861682547753538\n",
      "% Avg accuracy for n_min 25 : 85.30736583985666\n",
      "% Avg std of accuracy for n_min 25 : 1.1367235446124628\n"
     ]
    }
   ],
   "source": [
    "n_splits=10 #10 folds\n",
    "\n",
    "n_min_tests = [5,10,15,20,25] #For these different n_min parameters\n",
    "for n_min in n_min_tests:\n",
    "    \n",
    "    kf = KFold(n_splits) #Use Kfolds to generate the test folds.\n",
    "    count = 0\n",
    "    accuracy = []\n",
    "    for train, test in kf.split(data):\n",
    "        #Get the training data ready for each fold\n",
    "        training_data = []\n",
    "        training_label = []\n",
    "        for i in train:\n",
    "            training_data.append(data[i])\n",
    "            training_label.append(label[i])\n",
    "        #Create our model for each fold, \n",
    "        kfoldmodel = classificationTree(n_min)\n",
    "        kfoldmodel.fit(training_data, training_label)\n",
    "\n",
    "        #Predict on the test labels and collect results.\n",
    "        test_label = []\n",
    "        test_label_predicted = []\n",
    "        for i in test:\n",
    "            test_label.append(label[i])\n",
    "            test_label_predicted.append(kfoldmodel.predict(data[i]))\n",
    "\n",
    "        #Calculate accuracy\n",
    "        total = len(test_label)\n",
    "        correct = 0\n",
    "        for i in range(total):\n",
    "            if(test_label[i] == test_label_predicted[i]):\n",
    "                correct += 1\n",
    "        count += 1\n",
    "        accuracy.append((correct*100)/total)\n",
    "\n",
    "    accuracy = np.array(accuracy)\n",
    "    print(\"% Avg accuracy for n_min \"+str(n_min)+\" :\", np.mean(accuracy))\n",
    "    print(\"% Avg std of accuracy for n_min \"+str(n_min)+\" :\", np.std(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modify regression Tree from classification tree whereby:\n",
    "#Early stopping criteria: if instance is < n_min, return the mean feature value.\n",
    "#Use variance instead of information_gain.\n",
    "class regressionTree():\n",
    "    def __init__(self, n_min=5): #Create the tree with leaf size of n_min % of total data.\n",
    "        self.n_min = n_min\n",
    "        self.root = {}\n",
    "\n",
    "    #Splits into two groups for discreet data.\n",
    "    def __splitGroupDiscreet(self, data, axis, name):\n",
    "        #Place data into respective groups based on name.\n",
    "        group1 = []\n",
    "        group2 = []\n",
    "        for entry in data:\n",
    "            if entry[axis] == name:\n",
    "                group1.append(entry)\n",
    "            else:\n",
    "                group2.append(entry)\n",
    "        return(group1, group2)\n",
    "    \n",
    "    #Information gain of splitting on data[axis] = name\n",
    "    def __infoGainDiscreet(self, data, axis, name):\n",
    "        group1, group2 = self.__splitGroupDiscreet(data, axis, name)\n",
    "        group1_entropy = self.__entropy(group1) #Calculate the entropies\n",
    "        group2_entropy = self.__entropy(group2) #Calculate the entropies\n",
    "        resultEntropy = (len(group1)/len(data))*group1_entropy + (len(group2)/len(data))*group2_entropy #Ave entropy\n",
    "        return(self.__entropy(data) - resultEntropy)\n",
    "    \n",
    "    #Splits into two groups for continuous data.\n",
    "    def __splitGroupContinuous(self, data, axis, min_val):\n",
    "        group1 = []\n",
    "        group2 = []\n",
    "        for entry in data:\n",
    "            if len(min_val) > 0 and float(entry[axis]) > float(min_val):\n",
    "                group1.append(entry)\n",
    "            else:\n",
    "                group2.append(entry)\n",
    "                \n",
    "        return(group1, group2)\n",
    "    \n",
    "    #Information gain of splitting on value > min_val\n",
    "    def __infoGainContinuous(self, data, axis, min_val):\n",
    "        group1, group2 = self.__splitGroupContinuous(data, axis, str(min_val))\n",
    "        group1_entropy = self.__entropy(group1) #Calculate the entropies\n",
    "        group2_entropy = self.__entropy(group2) #Calculate the entropies\n",
    "        resultEntropy = (len(group1)/len(data))*group1_entropy + (len(group2)/len(data))*group2_entropy #Ave entropy\n",
    "        return(self.__entropy(data) - resultEntropy)\n",
    "    \n",
    "    #For resuable code with classification, we refer to variance as entropy.\n",
    "    def __entropy(self, group):\n",
    "        #If there are no values, we simply don't have variance.\n",
    "        if(len(group) == 0):\n",
    "            return(0)\n",
    "        group = np.array(group, dtype=float)\n",
    "        return(np.var(group[:,-1])) #Return the varance of this group\n",
    "\n",
    "    \n",
    "    #Finds the feature to split with the highest information gain.\n",
    "    #For discreet data, we use the median as the splitting category.\n",
    "    #For continuous data, we use the average\n",
    "    #Returns the column(feature), value\n",
    "    def __find_best_feature(self, data):\n",
    "        highest_info_gain = 0.0 #This tracks the highest we found\n",
    "        highest_column = 0 #This tracks the feature we should split by.\n",
    "        splitby = \"\" #This tracks the name (discreet) or min_split (continuous)\n",
    "        \n",
    "        for i in range(len(data[0])-1): #For each feature (column) minus label.\n",
    "            #Determine the type of feature.\n",
    "            featuretype = \"\"\n",
    "            if 'int' in str(type(data[0][i])) or 'float' in str(type(data[0][i])):\n",
    "                featuretype = \"continuous\"\n",
    "            elif 'str' in str(type(data[0][i])):\n",
    "                featuretype = \"discreet\"\n",
    "            \n",
    "            #If our feature is discreet, we check info gain on each name(type)\n",
    "            if featuretype == \"discreet\":\n",
    "                #Get only unique categories.\n",
    "                possibility = set()\n",
    "                for j in range(len(data)):\n",
    "                    possibility.add(data[j][i])\n",
    "                #Now iterate over unique names\n",
    "                for name in possibility:\n",
    "                    info_gain = self.__infoGainDiscreet(data, i, name)\n",
    "                    if(info_gain > highest_info_gain): #If we exceeded existin info_gain\n",
    "                        highest_info_gain = info_gain #Update highest values.\n",
    "                        highest_column = i\n",
    "                        splitby = name\n",
    "\n",
    "            #If the feature is continuous:\n",
    "            #\"For determining the optimal threshold for splitting you will need to search over\n",
    "            #all possible thresholds for a given feature\". This is slow. Esp for tasks \n",
    "            #such as spambase with 4k+ values over 20+ features\n",
    "            #We will use the mean value instead rather than search over the range.\n",
    "            elif featuretype == \"continuous\":\n",
    "                mean = 0\n",
    "                for j in range(len(data)):\n",
    "                    mean += data[j][i]\n",
    "                mean = (mean/len(data))\n",
    "                info_gain = self.__infoGainContinuous(data, i, mean)\n",
    "                if(info_gain > highest_info_gain): #If we exceeded existin info_gain\n",
    "                    highest_info_gain = info_gain #Update highest values.\n",
    "                    highest_column = i\n",
    "                    splitby = str(mean)\n",
    "\n",
    "        return(highest_column, splitby, featuretype)\n",
    "        \n",
    "    \n",
    "    #Given a node, find it's correct splits, and assign values.\n",
    "    def __splitNode(self, node):\n",
    "        \n",
    "        #Find the optimal splitting strategy.\n",
    "        column, splitby, featuretype = self.__find_best_feature(node[\"data\"])\n",
    "        node[\"column\"] = column\n",
    "        node[\"splitby\"] = splitby\n",
    "        \n",
    "        #Obtain the splits into group1 and group2\n",
    "        group1 = None\n",
    "        group2 = None\n",
    "        if featuretype == \"discreet\":\n",
    "            node[\"featuretype\"] = \"discreet\"\n",
    "            group1, group2 = self.__splitGroupDiscreet(node[\"data\"], column, splitby)\n",
    "        else:\n",
    "            node[\"featuretype\"] = \"continuous\"\n",
    "            group1, group2 = self.__splitGroupContinuous(node[\"data\"], column, splitby)\n",
    "        \n",
    "        #If one group has size <= self.n_min, we stop.\n",
    "        #Also stop if all features have the same value.\n",
    "        #Store the \"mode\" feature in the yes/no.\n",
    "        if len(group1) == 0:\n",
    "            node[\"yes\"] = None\n",
    "        elif len(group1) <= self.n_min: #If we have fewer than our minimum\n",
    "            l = [float(x[-1]) for x in group1]\n",
    "            node[\"yes\"] = sum(l)/float(len(l)) #The mean feature prediction is our regression value.\n",
    "        else: #If we still need to continue, then we recurse.\n",
    "            node[\"yes\"] = {\"data\":group1} \n",
    "            self.__splitNode(node[\"yes\"]) #Recurse on this side.\n",
    "\n",
    "        if len(group2) == 0:\n",
    "            node[\"no\"] = None\n",
    "        elif len(group2) <= self.n_min:\n",
    "            l = [float(x[-1]) for x in group1]\n",
    "            node[\"no\"] = sum(l)/float(len(l))\n",
    "        else:\n",
    "            node[\"no\"] = {\"data\":group2}\n",
    "            self.__splitNode(node[\"no\"]) #Recurse on this side.\n",
    "        return\n",
    "\n",
    "        \n",
    "    #Fit the data into our tree.\n",
    "    #Continuous data should be numerical. Categorical should be string.\n",
    "    def fit(self, data, label):\n",
    "        data = copy.deepcopy(data)\n",
    "        label = copy.deepcopy(label)\n",
    "        #Attach the labels to the end of our data.\n",
    "        for i in range(len(data)):\n",
    "            data[i].append(label[i])\n",
    "        self.labels = list(set(label))   #These are the unique labels.\n",
    "        self.size = len(data) #size of our dataset\n",
    "        self.n_min = (self.n_min * self.size)/100 #Minimum leaf size.\n",
    "        \n",
    "        #Use ID3 algorithm. \n",
    "        #1. Create a root node.\n",
    "        self.root = {\"data\":data}\n",
    "        #Recursively split this node by finding it's left and right.\n",
    "        self.__splitNode(self.root)\n",
    "        \n",
    "    #Recursively querys the node until we get our answer.\n",
    "    def __checkValue(self, node, query):\n",
    "        answer = \"\"\n",
    "        if(node[\"featuretype\"] == \"continuous\"): #Get the featureType.\n",
    "            if(query[node[\"column\"]] > float(node[\"splitby\"])): #Do the corresponding test.\n",
    "                #Check if this node is the final.\n",
    "                if \"dict\" in str(type(node[\"yes\"])): \n",
    "                    answer = self.__checkValue(node[\"yes\"], query)\n",
    "                else:\n",
    "                    answer = node[\"yes\"]\n",
    "            else:\n",
    "                #Check if this node is the final.\n",
    "                if \"dict\" in str(type(node[\"no\"])): \n",
    "                    answer = self.__checkValue(node[\"no\"], query)\n",
    "                else:\n",
    "                    answer = node[\"no\"]\n",
    "        elif(node[\"featuretype\"] == \"discreet\"):\n",
    "            if(query[node[\"column\"]] == node[\"splitby\"]):\n",
    "                #Check if this node is the final.\n",
    "                if \"dict\" in str(type(node[\"yes\"])): \n",
    "                    answer = self.__checkValue(node[\"yes\"], query)\n",
    "                else:\n",
    "                    answer = node[\"yes\"]\n",
    "            else:\n",
    "                #Check if this node is the final.\n",
    "                if \"dict\" in str(type(node[\"no\"])): \n",
    "                    answer = self.__checkValue(node[\"no\"], query)\n",
    "                else:\n",
    "                    answer = node[\"no\"]\n",
    "        return(answer)\n",
    "\n",
    "    #The driver to call self.__checkValue with.\n",
    "    def predict(self, query):\n",
    "        return(self.__checkValue(self.root, query))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicing boston suburb house prices with a decision tree (housing.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the iris dataset.\n",
    "data = loadData(\"housing.csv\")\n",
    "shuffle(data) #Shuffle our data\n",
    "label = [x[-1] for x in data] #Take out the labels. #This is median price in thousands.\n",
    "data = [x[:-1] for x in data]\n",
    "\n",
    "#Set the correct data types.\n",
    "for i in range(len(data)):\n",
    "    for j in range(len(data[0])):\n",
    "        data[i][j] = float(data[i][j])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create and fit our regression tree.\n",
    "tree = regressionTree(n_min = 5) #n_min = minimum percentage of data as leafnode size.\n",
    "tree.fit(data,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.550000000000004"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict one of our values\n",
    "#['0.02985', '0', '2.18', '0', '0.458', '6.43', '58.7', '6.0622', '3', '222', '18.7', '394.12', '5.21', '28.7']\n",
    "tree.predict([0.02985, 0, 2.18, 0, 0.458, 6.43, 58.7, 6.0622, 3, 222, 18.7, 394.12, 5.21])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the MSE accuracy of the model using 10 folds for various n_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg mean squared error for n_min 5 : 36.112191764705884\n",
      "Avg std for this MSE for n_min5 : 8.315048012229902\n",
      "Avg mean squared error for n_min 10 : 40.300452156862754\n",
      "Avg std for this MSE for n_min10 : 12.404878974774986\n",
      "Avg mean squared error for n_min 15 : 41.04351803921569\n",
      "Avg std for this MSE for n_min15 : 12.907606399568813\n",
      "Avg mean squared error for n_min 20 : 39.98316196078431\n",
      "Avg std for this MSE for n_min20 : 12.021595019127043\n",
      "Avg mean squared error for n_min 25 : 93.21851921568627\n",
      "Avg std for this MSE for n_min25 : 33.11527494067931\n"
     ]
    }
   ],
   "source": [
    "n_splits=10 #10 folds\n",
    "\n",
    "n_min_tests = [5,10,15,20,25] #For these different n_min parameters\n",
    "for n_min in n_min_tests:\n",
    "    kf = KFold(n_splits) #Use Kfolds to generate the test folds.\n",
    "    count = 0\n",
    "    accuracy = []\n",
    "    for train, test in kf.split(data):\n",
    "        #Get the training data ready for each fold\n",
    "        training_data = []\n",
    "        training_label = []\n",
    "        for i in train:\n",
    "            training_data.append(data[i])\n",
    "            training_label.append(label[i])\n",
    "        #Create our model for each fold, \n",
    "        kfoldmodel = classificationTree(n_min)\n",
    "        kfoldmodel.fit(training_data, training_label)\n",
    "\n",
    "        #Predict on the test labels and collect results.\n",
    "        test_label = []\n",
    "        test_label_predicted = []\n",
    "        for i in test:\n",
    "            test_label.append(label[i])\n",
    "            test_label_predicted.append(kfoldmodel.predict(data[i]))\n",
    "\n",
    "        #Calculate accuracy with MSR\n",
    "        total = len(test_label)\n",
    "        error = 0\n",
    "        for i in range(total):\n",
    "            error +=  (float(test_label[i]) - float(test_label_predicted[i])) ** 2 #Square error\n",
    "        accuracy.append(error/total) #Accuracy is the mean of this.\n",
    "        count += 1\n",
    "\n",
    "    accuracy = np.array(accuracy)\n",
    "    print(\"Avg mean squared error for n_min \"+str(n_min)+\" :\", np.mean(accuracy))\n",
    "    print(\"Avg std for this MSE for n_min\"+str(n_min)+\" :\", np.std(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that MSE increases slightly as n_min increases. However, past a certain n_min, the MSE dramatically increases and the algorithm beings to perform poorly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
